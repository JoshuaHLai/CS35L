For this lab, we first start out downloading the "coreutils 8.6" by using the command "wget http://www.gnu.org/software/coreutils/"
which was then saved as the file "index.html" to my local directory.  After running the command "sort --version" I found that the
system is using "sort (GNU coreutils) 8.22" which is sufficient since the spec says as long as it is higher than version 8.6.

To create the file with 10,000,000 random double-precision floating point numbers, I had to look up some documentation on the
command "od" , "sed" , and "tr".  Since a "double" number contains 8 bytes, I did some simple math and came to the conclusion
that 10,000,000 doubles is equivalent to 80,000,000 bytes. Once doing so, I ran the following command:
"od -An -tf -N 80000000 < /dev/urandom | tr -a ' ' '\n' > hugeFile.txt"

At the bottom of the spec, it stated to make sure my PATH environment variable is set properly, so I used the following command:
"export PATH=/usr/local/cs/bin"
"echo $PATH"

AFter reading documentation using "info sort" and "sort --help" I learned how to use the "--parallel" flag for sort in order to test
the time of the function. To test the time and efficiency of threading with 1, 2, 4, and 8 threads, I used the following commands.
Along with the commands the output is also shown below.

time -p sort -g --parallel=1 hugeFile.txt > /dev/null
real 205.35
user 205.03
sys 0.31

time -p sort -g --parallel=2 hugeFile.txt > /dev/null
real 103.34
user 197.55
sys 0.32

time -p sort -g --parallel=4 hugeFile.txt > /dev/null
real 57.59
user 198.12
sys 0.45

time -p sort -g --parallel=8 hugeFile.txt > /dev/null
real 37.24
user 200.45
sys 0.55

Looking at the results, it is clear that the more threads that are utilized, the faster the command will run.  If using only
one thread, the program takes a very long time to run through.  As the number of threads increased, the command took a shorter
amount of time.  For the fun of it, I set the number threads to 32 and 128 to see if there is still an increase in speed.

time -p sort -g --parallel=32 hugeFile.txt > /dev/null
real 25.20
user 325.62
sys 0.94

time -p sort -g --parallel=128 hugeFile.txt > /dev/null
real 24.23
user 326.10
sys 1.96

Looking at the results, it seems that at some point, the number of threads you have becomes useless as the results become very
similar.
